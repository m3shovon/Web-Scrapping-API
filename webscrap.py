# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kub7lLK4z73cltD-0m5rkoSjVxIJXGnT
"""

# !pip install scrapy beautifulsoup4 transformers sentence-transformers faiss-cpu fastapi uvicorn nest-asyncio

import requests
from bs4 import BeautifulSoup

def scrape_website(base_url):
    # Get the webpage content
    response = requests.get(base_url)
    if response.status_code != 200:
        print("Failed to fetch the website.")
        return None

    # Parse the HTML content
    soup = BeautifulSoup(response.text, 'html.parser')

    # Extract meaningful text (e.g., from <p>, <h1>, <h2>, etc.)
    content = []
    for tag in soup.find_all(['p', 'h1', 'h2', 'h3']):
        if tag.text.strip():
            content.append(tag.text.strip())

    return content

# Example usage
base_url = "https://theicthub.com/"  # Replace with your target website
website_content = scrape_website(base_url)
print("Scraped Content:", website_content[:115])  # Preview some content

from sentence_transformers import SentenceTransformer
import numpy as np
import faiss

# Load pre-trained SentenceTransformer model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Generate embeddings for website content
if website_content:
    embeddings = model.encode(website_content, convert_to_tensor=False)

    # Convert to a NumPy array
    embeddings = np.array(embeddings)

    # Initialize FAISS index for similarity search
    index = faiss.IndexFlatL2(embeddings.shape[1])  # L2 distance
    index.add(embeddings)  # Add embeddings to the index

print(f"Created FAISS index with {len(embeddings)} entries.")

def search_content(query, top_k=3):
    # Generate query embedding
    query_embedding = model.encode([query], convert_to_tensor=False)

    # Search for similar entries in the FAISS index
    distances, indices = index.search(np.array(query_embedding), top_k)

    # Retrieve top-k results
    results = [website_content[idx] for idx in indices[0]]
    return results

# Example usage
query = "Address in theicthub?"
answers = search_content(query)
print("Top Answers:")
for i, answer in enumerate(answers, 1):
    print(f"{i}. {answer}")